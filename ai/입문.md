

# 인공지능 입문

## 튜링 테스트
벽 건너편의 상대방에게 질문을 하고
질문에 대해서 답변을 받는다.
이 답을 보고 컴퓨터인지 사람인지 모르겠다 라고 하면
튜링 테스트를 통과한거다 라고 한다. ai 이다.

AI = human-level intelligence
왜 인간 수준으로 정의 하느냐? 최고 수준으로 할 수 있지 않느냐

What can AI do for you?

딥러닝에 의해서 인지하는 기술은 거의 사람만큼 가능해졌다

- 사물을 인지하는 기능
- Autonomous Driving - tesla, google
- handwriting recognition
- 자연어 처리 - Sentiment Analysis : 영화를 리뷰하는 글을 보고 이 글이 Positive 인지 negative 인지 판단하는
단어를 기준으로 positive negative 점수를 주고 판단했었다 => 문맥을 판단할 수 없기 때문에 어렵다
딥러닝으로 인해 많이 향상되었다.
- Machine Translations

자연어처리가 인공지능에서 가장 어려운 분야가 아닌가 싶다

## 인간 vs Machine
1997 : Deep Blue (chess) 
-> 슈퍼컴퓨터를 사용해서 많은 경우의 수를 사람보다 빠르게 처리
-> 검색, tree search 로 가능
2011 : IBM watson (jeopardy)
2016 : AlphaGo (바둑)
-> 바둑은 체스에 비해서 경우의 수가 너무 많다
-> 기계 학습을 도입, 검색 알고리즘


## 인공지능에 대한 걱정
인공지능이 인간을 지배하게 될까?
아직까지는 그런 수준은 아니다.
알파고의 경우 자신이 두고 있는 상대가 누구인지 상황이나 그런것들에 대해서는 전혀 알 수가 없다.
본능이나 감정을 심어주는 거는 또 다른 이야기다. 아직은 불가능하다.

## AI Winter
기술적인 한계에 부딪히면 암흑기가 온다.
지금은 전성기.... 

## Virtual Assistants
시리, 빅스비

아마존의 에코라는 시스템이 있음
너무 잘 알아들음, 음악이 나오는 상황에서 자기 목소리를 캐치해서 명령을 수행함
대단한 성능을 가지고 있음

## Dialog System
자연어 처리 인터페이스
2016 ten breakthrough technology from MIT technology review
chat bot
tutor robot
기술은 많이 발전했지만 제공하기에는 여러가지 제도적인 문제들이 있다.

## Many AI Applications
많은 응용 분야들이 있음

# Two sources of complexity
응용 분야들의 공통점
- Computational Complexity
대부분의 AI 문제들은 NP-hard 이다. 최적의 솔루션이 있지만 시간이 너무 오래 걸리는 문제
바둑의 경우 361^200 의 경우의 수가 있음 (우주의 원자 개수)
종이를 50번접으면 달나라를 왔다 갔다 할 수 있는 높이다. 2^50 
- Information Complexity
많은 양의 데이터가 필요한 문제

# Resources
빠른 컴퓨터, Big data, Machine Learning Algorithm
- alphago : 30 million taining data, tensor processing unit,  deep & reinforcement learning
- NVIDIA Auto Driving : Vision data by driving tens of thousands miles, GPU, Deep Learning
- Google Translation : Billions of translation data, IBM's linguistic approach fails

# 인공지능에서의 문제 해결은?
Real-world task => Modeling => Formal task(model) => Algorithms => Program

## real-world task
1000 개의 웹페이지 에서 10 개의 가장 관련있는 웹 페이지를 찾는다.
## Formal task
- input : list(L) and a function(f)
- output : k highest-scoring elements
### Two algorithms
1. 스캔하면 가장 큰 값을 찾고 다음 큰 값을 찾는다. (n*k 시간이 걸린다.)
2. Sort L base on f, return first k elements (n*log n 시간이 걸린다.)
### Modeling
L = list of web pages
f(x) = 10 * QueryMatch(x) + 3* PageRank(x)
사람은 feature 만 주어지고 가중치를 머시러닝(빅데이터)을 통해 찾아 function 을 완성한다.
feature : QueryMatch, PageRank
가중치 : 10, 3

## Modeling and algorithms
인공지능에서 배우는 것
- Type of models...
- Art of modeling...
- Developing Algorithms...

### Reflex
가장 단순한 model
영화 리뷰를 통해 긍정과 부정을 판단하는 것을 모델링한다고 하면
단어들을 긍정과 부정으로 나누고 단어들로 판단한다.
cliches 를 포함하고 있으면 -10
promise 를 포함하고 있으면 +5 를 줘서 마지막에 점수로 판단한다.
key idea : linear classifier
f(x) = sign(w1o1(x) + ... + wdod(x))

### Machine learning approadch
- Tranning examples => Learning algorithm => Simple Program with parameter
Movie review 와 결과를 준비해야한다. labeling 을 해줘야 한다.

### 모델의 성능에 대한 판단
Key idea: generalization
training data 를 가지고 판단한 label 과 실제 label 을 비교한다.
future test 가 실제 성능에 대한 지표

### State-based Models
Solutions are represented as paths through a graph
Key idea: state
A state captures all the relevant information about the past in order to act optimally in the future
alphago 도 state-based model 을 사용했다
- Search problems
- Markov decision processes
- adversarial game
- Crawling robot


### Variables-based Models
state, value, constraint
Constraint Satisfaction Problem (CSP) : hard constraints
Bayesian networks : soft dependencies (tracking cars from sensors)

### Topic Modeling (logic)
대용량의 문서들을 주제별로 ..

## Types of prediction tasks
- Binary classificatoin : 둘 중 하나 판단
- Regression : 값을 예측
- Multiclass classification : 사진을 보고 고양이냐
- Ranking : permutation
- structured prediction : 새로운 데이터를 만들어 내는 것...

# Modeling f => Optimization 
Modeling 은 결국 optimization 으로 바뀐다
## Problem : predicting exam score y
- x: # of hours studying
- y: exam score
y = wx
우리가 학습하고자 하는건 w
F(w) = sigma(xiw - yi)^2, 최소화 시키는 값

데이터로부터 최적의 값을 찾는 과정, Modeling
어떤 loss function 을 최소화는 값을 찾는 거를 optimization 이라고 한다.

# Features
Modeling 할 때 사람이 제공하는 것 feature 와 data를 제공해야하는데,,
feature 를 잘 제공해줘야 한다.
예전에는 feature 를 직접 계산해서 구했지만
deep learning 은 feature 를 스스로 학습하기 때문에 요즘에는 거의 정확해졌다

## Feature extractor
feature vector notation

# Hypothesis class
가능한 f 의 set 이 있다고 가정해보죠..
feature 를 define 하는 순간 인공지능 알고리즘이 생성해낼 수 있는 f 숫자는 확 줄어든다.

feature 를 학습한다는게 무엇인가?
# Neural networks
Example: predicting car collision
자동차의 위치만 가지고는 linear function 으로 충돌 여부를 알 수 없다. 
두개의 linear function 을 연결해서 다시 새로운 Linear function 으로

primitive 한 feature 로부터 high level feature 를 만들어가는게 deep learning

# overfitting
학습한 데이터에서는 성능이 좋은데 학습하지 않은 새로운 데이터에서는 성능이 안좋을 수 있다
모델이 너무 복잡하면 안되고 모델을 단순하게 하기 위한 많은 방법들이 있다.

# Summary
- Modeling f => Optimization
- Features
- Generalization






